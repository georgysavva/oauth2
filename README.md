# ELSA Challenge for backend/integrations developer

This is Elsa’s Coding Exercise. It allows Elsa to assess candidates’ ability to develop well structured, maintainable and useful code. The code does not need to be fancy, but needs to represent the coding skills of the candidate, and it will be the basis for discussions on the technical interview. 

You can start this any time you like. It should not take you more than 1 day to code. You can turn it in at any time. As you think about this exercise, please feel to ask questions!

Please Fork this repo and submit your solution in the fork

Happy coding!

## Problem 
{state problem}

## Basic Functionality

- A user should be able to create a note by starting to record audio
- Notes should be created with the audio and transcribed using some ASR/Voice-to-Text service (e.g. Google ASR API or others)
- Users should be able to edit the transcribed text
- Users should be able to see a list of their notes
- A location should automatically captured and saved metadata for the note
- Users should be able to select a note and view the transcribed/edited text, playback the voice note, and view the location of where the voice note was originally taken.

## Optional Extras 

(These are ideas. You are not obligated to implement any of these. We prefer to see an app with limited feature set and quality code, as opposed to an app with more features that is hacked together)

- Allow the user to attach a photo to a note
- Allow the user to attach multiple photos to a note
- Allow the user to share the audio + text via iMessage and other services that support that media
- Allow the user to share the audio + text + attached image(s) via iMessage and other services that support that media
- Map view of all the notes with a tap to select a note and view it
- Create an iMessage app that, when activated in iMessage, shows a list of taken notes and allows for sharing
- Create an AppleWatch app extension that lists the note items with their content